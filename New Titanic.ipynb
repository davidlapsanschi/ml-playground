{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "TITANIC_FOLDER_PATH = path = os.path.join(\"datasets\", \"titanic\")\n",
    "\n",
    "\n",
    "def load_titanic_data(filename):\n",
    "    file_path = os.path.join(TITANIC_FOLDER_PATH, filename)\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "train_data = load_titanic_data(\"train.csv\")\n",
    "test_data = load_titanic_data(\"test.csv\")\n",
    "\n",
    "titanic_data = train_data.drop(\"Survived\", axis=1)\n",
    "titanic_labels = train_data[\"Survived\"].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('category_encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        new_data = X.copy()\n",
    "        new_data.loc[new_data[\"Fare\"] == 0, \"Fare\"] = np.NaN\n",
    "\n",
    "        new_data['Title'] = new_data['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "        new_data = new_data.replace({\n",
    "            'Title': {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'the Countess': 'Mrs', 'Don': 'Mr', 'Mme': 'Mrs',\n",
    "                      'Ms': 'Miss', 'Lady': 'Miss', 'Sir': 'Mr', 'Capt': 'Mr', 'Jonkheer': 'Mr', 'Dona': 'Mrs',\n",
    "                      'Dr': 'Other',\n",
    "                      'Rev': 'Other'}\n",
    "        })\n",
    "\n",
    "        new_data['Ticket_2letter'] = new_data['Ticket'] \\\n",
    "            .apply(lambda ticket: ticket[:2])\n",
    "        new_data['Ticket_len'] = new_data['Ticket'] \\\n",
    "            .apply(lambda ticket: len(ticket))\n",
    "\n",
    "        new_data['Cabin_sum'] = new_data[~new_data['Cabin'].isna()]['Cabin'] \\\n",
    "            .apply(lambda cabin: len(str(cabin).split()))\n",
    "\n",
    "        new_data['Deck'] = new_data[~new_data['Cabin'].isna()]['Cabin'] \\\n",
    "            .apply(lambda cabin: str(cabin)[:1])\n",
    "        idx = new_data[new_data['Deck'] == 'T'].index\n",
    "        new_data.loc[idx, 'Deck'] = 'A'\n",
    "        new_data['Deck'] = new_data['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
    "        new_data['Deck'] = new_data['Deck'].replace(['D', 'E'], 'DE')\n",
    "        new_data['Deck'] = new_data['Deck'].replace(['F', 'G'], 'FG')\n",
    "\n",
    "        new_data['Family_Size'] = new_data['SibSp'] + new_data[\n",
    "            'Parch'] + 1  #the 1 is the person. he's part of the family\n",
    "        new_data['Family_Type'] = pd.cut(new_data['Family_Size'], [0, 1, 4, 7, 11],\n",
    "                                         labels=['Alone', 'Small', 'Big', 'Very Big'])\n",
    "\n",
    "        new_data[\"Age_Group\"] = pd.cut(new_data[\"Age\"], [0, 15, 30, 45, 60, 120],\n",
    "                                       labels=['Child', 'Young Adult', 'Adult', 'Old', 'Very Old'])\n",
    "\n",
    "        new_data[\"Fare_Group\"] = pd.qcut(new_data[\"Fare\"], 4, labels=['Cheap', 'Standard', 'Expensive',\n",
    "                                                                      'Luxury'])\n",
    "\n",
    "        new_data['Ticket_Frequency'] = new_data.groupby('Ticket')['Ticket'].transform('count')\n",
    "\n",
    "        return new_data\n",
    "\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[num_attribs + cat_attribs]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma='auto')\n",
    "\n",
    "\n",
    "def train_models_with_features(data, labels, num_attribs, cat_attribs):\n",
    "    full_transformer_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs)\n",
    "    ])\n",
    "\n",
    "    full_pipeline = Pipeline([\n",
    "        ('feature_adder', FeatureAdder()),\n",
    "        ('feature_selector', FeatureSelector()),\n",
    "        ('transformer', full_transformer_pipeline),\n",
    "    ])\n",
    "\n",
    "    titanic_data_prepared = full_pipeline.fit_transform(data)\n",
    "    forest_scores = cross_val_score(forest_clf, titanic_data_prepared, labels, cv=10)\n",
    "    print(\"Forest mean:\", forest_scores.mean())\n",
    "    svm_scores = cross_val_score(svm_clf, titanic_data_prepared, labels, cv=10)\n",
    "    print(\"SVM mean:\", svm_scores.mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "num_attribs = [\"Cabin_sum\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "cat_attribs = [\"Sex\", \"Title\", \"Embarked\", \"Age_Group\", \"Fare_Group\", \"Pclass\", \"Deck\", \"Family_Type\"]\n",
    "\n",
    "full_transformer_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_adder', FeatureAdder()),\n",
    "    ('feature_selector', FeatureSelector()),\n",
    "    ('transformer', full_transformer_pipeline),\n",
    "])\n",
    "\n",
    "titanic_train_prepared = full_pipeline.fit_transform(titanic_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "data": {
      "text/plain": "(891, 34)"
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train_prepared.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_55 (Dense)            (None, 11)                440       \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 11)                132       \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 11)                0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 11)                132       \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 11)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 5)                 60        \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=39, units=11, kernel_initializer=\"uniform\"))\n",
    "classifier.add(Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\"))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(activation=\"relu\", units=11, kernel_initializer=\"uniform\"))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(activation=\"relu\", units=5, kernel_initializer=\"uniform\"))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "classifier.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.15126507, -0.56573646,  0.43279337, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.15126507,  0.66386103,  0.43279337, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.15126507, -0.25833709, -0.4745452 , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-0.15126507, -0.1046374 ,  0.43279337, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.15126507, -0.25833709, -0.4745452 , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.15126507,  0.20276197, -0.4745452 , ...,  0.        ,\n         0.        ,  0.        ]])"
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train_prepared"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 1s 4ms/step - loss: 0.6903 - accuracy: 0.6096 - val_loss: 0.6841 - val_accuracy: 0.6425\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.6589 - accuracy: 0.6096 - val_loss: 0.5670 - val_accuracy: 0.6425\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5832 - accuracy: 0.6096 - val_loss: 0.4932 - val_accuracy: 0.6425\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.6096 - val_loss: 0.4905 - val_accuracy: 0.6425\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.7008 - val_loss: 0.4749 - val_accuracy: 0.8492\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7781 - val_loss: 0.4839 - val_accuracy: 0.8547\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5467 - accuracy: 0.7992 - val_loss: 0.4605 - val_accuracy: 0.8603\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7921 - val_loss: 0.4625 - val_accuracy: 0.8547\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7935 - val_loss: 0.4509 - val_accuracy: 0.8436\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.8090 - val_loss: 0.4442 - val_accuracy: 0.8492\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.8216 - val_loss: 0.4467 - val_accuracy: 0.8547\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.8160 - val_loss: 0.4454 - val_accuracy: 0.8547\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.8160 - val_loss: 0.4567 - val_accuracy: 0.8436\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.8076 - val_loss: 0.4657 - val_accuracy: 0.8436\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.8174 - val_loss: 0.4317 - val_accuracy: 0.8547\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.8160 - val_loss: 0.4341 - val_accuracy: 0.8436\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.8301 - val_loss: 0.4368 - val_accuracy: 0.8492\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.8272 - val_loss: 0.4350 - val_accuracy: 0.8380\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.8174 - val_loss: 0.4344 - val_accuracy: 0.8436\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.8146 - val_loss: 0.4304 - val_accuracy: 0.8492\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.8216 - val_loss: 0.4265 - val_accuracy: 0.8436\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.8160 - val_loss: 0.4432 - val_accuracy: 0.8436\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.8230 - val_loss: 0.4172 - val_accuracy: 0.8436\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.8188 - val_loss: 0.4197 - val_accuracy: 0.8492\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.8174 - val_loss: 0.4269 - val_accuracy: 0.8436\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7992 - val_loss: 0.4315 - val_accuracy: 0.8380\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.8343 - val_loss: 0.4205 - val_accuracy: 0.8436\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.8104 - val_loss: 0.4256 - val_accuracy: 0.8380\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4998 - accuracy: 0.8076 - val_loss: 0.4197 - val_accuracy: 0.8436\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.8160 - val_loss: 0.4203 - val_accuracy: 0.8380\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8202 - val_loss: 0.4252 - val_accuracy: 0.8380\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.8188 - val_loss: 0.4301 - val_accuracy: 0.8436\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.8174 - val_loss: 0.4323 - val_accuracy: 0.8380\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.8160 - val_loss: 0.4458 - val_accuracy: 0.8380\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.8062 - val_loss: 0.4204 - val_accuracy: 0.8436\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.8076 - val_loss: 0.4234 - val_accuracy: 0.8380\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.8188 - val_loss: 0.4258 - val_accuracy: 0.8380\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.8216 - val_loss: 0.4126 - val_accuracy: 0.8436\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.8202 - val_loss: 0.4235 - val_accuracy: 0.8380\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.8132 - val_loss: 0.4190 - val_accuracy: 0.8436\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.8188 - val_loss: 0.4284 - val_accuracy: 0.8380\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.8146 - val_loss: 0.4184 - val_accuracy: 0.8492\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.8160 - val_loss: 0.4114 - val_accuracy: 0.8436\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.8202 - val_loss: 0.4285 - val_accuracy: 0.8380\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.8118 - val_loss: 0.4162 - val_accuracy: 0.8436\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.8315 - val_loss: 0.4292 - val_accuracy: 0.8436\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.8006 - val_loss: 0.4066 - val_accuracy: 0.8436\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.8048 - val_loss: 0.4052 - val_accuracy: 0.8436\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.8202 - val_loss: 0.4040 - val_accuracy: 0.8492\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.8160 - val_loss: 0.4130 - val_accuracy: 0.8380\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.8160 - val_loss: 0.4070 - val_accuracy: 0.8436\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.8090 - val_loss: 0.4208 - val_accuracy: 0.8436\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.8272 - val_loss: 0.4036 - val_accuracy: 0.8436\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.8160 - val_loss: 0.4065 - val_accuracy: 0.8436\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8104 - val_loss: 0.4026 - val_accuracy: 0.8436\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.8090 - val_loss: 0.4240 - val_accuracy: 0.8436\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.8160 - val_loss: 0.4053 - val_accuracy: 0.8436\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8287 - val_loss: 0.4133 - val_accuracy: 0.8492\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.8020 - val_loss: 0.4333 - val_accuracy: 0.8380\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.8006 - val_loss: 0.4206 - val_accuracy: 0.8492\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.8146 - val_loss: 0.4244 - val_accuracy: 0.8380\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.8090 - val_loss: 0.4098 - val_accuracy: 0.8492\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.8146 - val_loss: 0.4049 - val_accuracy: 0.8492\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7978 - val_loss: 0.4099 - val_accuracy: 0.8436\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.8258 - val_loss: 0.4117 - val_accuracy: 0.8324\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.8329 - val_loss: 0.4086 - val_accuracy: 0.8492\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7992 - val_loss: 0.4138 - val_accuracy: 0.8436\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.8048 - val_loss: 0.3990 - val_accuracy: 0.8380\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7992 - val_loss: 0.4051 - val_accuracy: 0.8492\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.8160 - val_loss: 0.4212 - val_accuracy: 0.8436\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.8216 - val_loss: 0.4136 - val_accuracy: 0.8380\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.8132 - val_loss: 0.3992 - val_accuracy: 0.8492\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.8160 - val_loss: 0.4096 - val_accuracy: 0.8492\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.8006 - val_loss: 0.4071 - val_accuracy: 0.8436\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7921 - val_loss: 0.4102 - val_accuracy: 0.8380\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8048 - val_loss: 0.4060 - val_accuracy: 0.8436\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4877 - accuracy: 0.8132 - val_loss: 0.4059 - val_accuracy: 0.8436\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7963 - val_loss: 0.4193 - val_accuracy: 0.8436\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8216 - val_loss: 0.4122 - val_accuracy: 0.8380\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8132 - val_loss: 0.4008 - val_accuracy: 0.8380\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.8076 - val_loss: 0.4129 - val_accuracy: 0.8436\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.8104 - val_loss: 0.4062 - val_accuracy: 0.8436\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.8357 - val_loss: 0.4011 - val_accuracy: 0.8436\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8230 - val_loss: 0.3981 - val_accuracy: 0.8436\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.8006 - val_loss: 0.4006 - val_accuracy: 0.8436\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.8076 - val_loss: 0.4029 - val_accuracy: 0.8436\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.8258 - val_loss: 0.4026 - val_accuracy: 0.8436\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.8020 - val_loss: 0.4075 - val_accuracy: 0.8436\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.8118 - val_loss: 0.4052 - val_accuracy: 0.8436\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8132 - val_loss: 0.4115 - val_accuracy: 0.8436\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8301 - val_loss: 0.4060 - val_accuracy: 0.8436\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.8329 - val_loss: 0.4202 - val_accuracy: 0.8436\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.8272 - val_loss: 0.4110 - val_accuracy: 0.8436\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.8188 - val_loss: 0.4092 - val_accuracy: 0.8436\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.8174 - val_loss: 0.4262 - val_accuracy: 0.8380\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.8258 - val_loss: 0.4087 - val_accuracy: 0.8436\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.8146 - val_loss: 0.4018 - val_accuracy: 0.8436\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.8188 - val_loss: 0.4050 - val_accuracy: 0.8436\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.8118 - val_loss: 0.4041 - val_accuracy: 0.8380\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.8132 - val_loss: 0.3950 - val_accuracy: 0.8436\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit(titanic_train_prepared, titanic_labels, batch_size=10, epochs=100,\n",
    "                         validation_split=0.2, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 6ms/step - loss: 0.5264 - accuracy: 0.7556 - val_loss: 0.3747 - val_accuracy: 0.8324\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.8329 - val_loss: 0.3505 - val_accuracy: 0.8547\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8413 - val_loss: 0.3438 - val_accuracy: 0.8492\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8624 - val_loss: 0.3418 - val_accuracy: 0.8659\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8666 - val_loss: 0.4000 - val_accuracy: 0.8603\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8567 - val_loss: 0.3418 - val_accuracy: 0.8547\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8680 - val_loss: 0.3876 - val_accuracy: 0.8603\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8722 - val_loss: 0.3715 - val_accuracy: 0.8603\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8806 - val_loss: 0.3943 - val_accuracy: 0.8436\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8848 - val_loss: 0.3910 - val_accuracy: 0.8659\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8862 - val_loss: 0.3927 - val_accuracy: 0.8547\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.8919 - val_loss: 0.3984 - val_accuracy: 0.8659\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8876 - val_loss: 0.4341 - val_accuracy: 0.8436\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8904 - val_loss: 0.3837 - val_accuracy: 0.8659\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8989 - val_loss: 0.4205 - val_accuracy: 0.8547\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.9045 - val_loss: 0.4066 - val_accuracy: 0.8659\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.9045 - val_loss: 0.4363 - val_accuracy: 0.8156\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.9045 - val_loss: 0.4296 - val_accuracy: 0.8380\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8961 - val_loss: 0.4120 - val_accuracy: 0.8547\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.9031 - val_loss: 0.4234 - val_accuracy: 0.8268\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.9115 - val_loss: 0.4674 - val_accuracy: 0.8492\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.9059 - val_loss: 0.4418 - val_accuracy: 0.8547\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9045 - val_loss: 0.4523 - val_accuracy: 0.8492\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.9129 - val_loss: 0.4442 - val_accuracy: 0.8547\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2462 - accuracy: 0.9059 - val_loss: 0.4697 - val_accuracy: 0.8603\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9003 - val_loss: 0.4490 - val_accuracy: 0.8380\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.9129 - val_loss: 0.5096 - val_accuracy: 0.8492\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9073 - val_loss: 0.4626 - val_accuracy: 0.8324\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9157 - val_loss: 0.5082 - val_accuracy: 0.8492\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9087 - val_loss: 0.5594 - val_accuracy: 0.8547\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9129 - val_loss: 0.5333 - val_accuracy: 0.8492\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9143 - val_loss: 0.5286 - val_accuracy: 0.8436\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2187 - accuracy: 0.9157 - val_loss: 0.4948 - val_accuracy: 0.8380\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9129 - val_loss: 0.5148 - val_accuracy: 0.8380\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9087 - val_loss: 0.5822 - val_accuracy: 0.8547\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9115 - val_loss: 0.5363 - val_accuracy: 0.8492\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9129 - val_loss: 0.5347 - val_accuracy: 0.8436\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9129 - val_loss: 0.5533 - val_accuracy: 0.8547\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9101 - val_loss: 0.5209 - val_accuracy: 0.8436\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2225 - accuracy: 0.9143 - val_loss: 0.5150 - val_accuracy: 0.8492\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9129 - val_loss: 0.5054 - val_accuracy: 0.8547\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9115 - val_loss: 0.5435 - val_accuracy: 0.8324\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9101 - val_loss: 0.5779 - val_accuracy: 0.8547\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9213 - val_loss: 0.5585 - val_accuracy: 0.8324\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9143 - val_loss: 0.5810 - val_accuracy: 0.8436\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9101 - val_loss: 0.5484 - val_accuracy: 0.8436\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2050 - accuracy: 0.9185 - val_loss: 0.5702 - val_accuracy: 0.8324\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2057 - accuracy: 0.9185 - val_loss: 0.5551 - val_accuracy: 0.8380\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9228 - val_loss: 0.6294 - val_accuracy: 0.8603\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2110 - accuracy: 0.9157 - val_loss: 0.5954 - val_accuracy: 0.8436\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9185 - val_loss: 0.5708 - val_accuracy: 0.8547\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9157 - val_loss: 0.6563 - val_accuracy: 0.8547\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9228 - val_loss: 0.6399 - val_accuracy: 0.8547\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9185 - val_loss: 0.5869 - val_accuracy: 0.8547\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1909 - accuracy: 0.9228 - val_loss: 0.6141 - val_accuracy: 0.8436\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9115 - val_loss: 0.5800 - val_accuracy: 0.8547\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9199 - val_loss: 0.6238 - val_accuracy: 0.8492\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2038 - accuracy: 0.9171 - val_loss: 0.6217 - val_accuracy: 0.8603\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9101 - val_loss: 0.6274 - val_accuracy: 0.8492\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9157 - val_loss: 0.6241 - val_accuracy: 0.8492\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2047 - accuracy: 0.9185 - val_loss: 0.6159 - val_accuracy: 0.8492\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1989 - accuracy: 0.9213 - val_loss: 0.6316 - val_accuracy: 0.8380\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9228 - val_loss: 0.6603 - val_accuracy: 0.8380\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.9242 - val_loss: 0.6578 - val_accuracy: 0.8547\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9185 - val_loss: 0.6223 - val_accuracy: 0.8492\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2003 - accuracy: 0.9185 - val_loss: 0.6570 - val_accuracy: 0.8603\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9199 - val_loss: 0.6409 - val_accuracy: 0.8547\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9199 - val_loss: 0.6698 - val_accuracy: 0.8603\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9213 - val_loss: 0.6447 - val_accuracy: 0.8324\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2158 - accuracy: 0.9157 - val_loss: 0.6241 - val_accuracy: 0.8547\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9143 - val_loss: 0.6287 - val_accuracy: 0.8380\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9256 - val_loss: 0.6673 - val_accuracy: 0.8324\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1912 - accuracy: 0.9228 - val_loss: 0.7085 - val_accuracy: 0.8547\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2029 - accuracy: 0.9185 - val_loss: 0.6459 - val_accuracy: 0.8268\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9242 - val_loss: 0.6788 - val_accuracy: 0.8268\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9242 - val_loss: 0.7015 - val_accuracy: 0.8324\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9270 - val_loss: 0.7592 - val_accuracy: 0.8436\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9228 - val_loss: 0.7204 - val_accuracy: 0.8324\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1947 - accuracy: 0.9199 - val_loss: 0.7136 - val_accuracy: 0.8324\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1855 - accuracy: 0.9185 - val_loss: 0.7271 - val_accuracy: 0.8436\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9213 - val_loss: 0.8028 - val_accuracy: 0.8436\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9171 - val_loss: 0.7157 - val_accuracy: 0.8380\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9256 - val_loss: 0.7677 - val_accuracy: 0.8492\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9270 - val_loss: 0.7566 - val_accuracy: 0.8436\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9213 - val_loss: 0.7137 - val_accuracy: 0.8436\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.9242 - val_loss: 0.7947 - val_accuracy: 0.8492\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1936 - accuracy: 0.9171 - val_loss: 0.7175 - val_accuracy: 0.8324\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9213 - val_loss: 0.7894 - val_accuracy: 0.8436\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9143 - val_loss: 0.8266 - val_accuracy: 0.8436\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9185 - val_loss: 0.7200 - val_accuracy: 0.8492\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9213 - val_loss: 0.7617 - val_accuracy: 0.8547\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.9256 - val_loss: 0.7752 - val_accuracy: 0.8492\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1800 - accuracy: 0.9242 - val_loss: 0.7881 - val_accuracy: 0.8436\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.9242 - val_loss: 0.7923 - val_accuracy: 0.8324\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1765 - accuracy: 0.9242 - val_loss: 0.8187 - val_accuracy: 0.8492\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1848 - accuracy: 0.9256 - val_loss: 0.8267 - val_accuracy: 0.8324\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9284 - val_loss: 0.8154 - val_accuracy: 0.8324\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9284 - val_loss: 0.8794 - val_accuracy: 0.8380\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1730 - accuracy: 0.9298 - val_loss: 0.8745 - val_accuracy: 0.8212\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9256 - val_loss: 0.8592 - val_accuracy: 0.8380\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1766 - accuracy: 0.9270 - val_loss: 0.8466 - val_accuracy: 0.8436\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9284 - val_loss: 0.8165 - val_accuracy: 0.8268\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.9284 - val_loss: 0.8467 - val_accuracy: 0.8268\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9115 - val_loss: 0.7348 - val_accuracy: 0.8380\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9087 - val_loss: 0.7539 - val_accuracy: 0.8380\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1945 - accuracy: 0.9171 - val_loss: 0.7886 - val_accuracy: 0.8380\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1748 - accuracy: 0.9284 - val_loss: 0.8064 - val_accuracy: 0.8324\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.9326 - val_loss: 0.8581 - val_accuracy: 0.8324\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9284 - val_loss: 0.8092 - val_accuracy: 0.8380\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9326 - val_loss: 0.8657 - val_accuracy: 0.8380\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1664 - accuracy: 0.9312 - val_loss: 0.8705 - val_accuracy: 0.8380\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9284 - val_loss: 0.9202 - val_accuracy: 0.8212\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9228 - val_loss: 0.8288 - val_accuracy: 0.8212\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9242 - val_loss: 0.8682 - val_accuracy: 0.8436\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1772 - accuracy: 0.9270 - val_loss: 0.8623 - val_accuracy: 0.8324\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.9284 - val_loss: 0.8893 - val_accuracy: 0.8436\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9312 - val_loss: 0.9204 - val_accuracy: 0.8324\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9256 - val_loss: 0.8951 - val_accuracy: 0.8436\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1695 - accuracy: 0.9312 - val_loss: 0.9556 - val_accuracy: 0.8380\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1669 - accuracy: 0.9312 - val_loss: 0.9635 - val_accuracy: 0.8268\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1678 - accuracy: 0.9326 - val_loss: 0.9403 - val_accuracy: 0.8380\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9228 - val_loss: 0.9561 - val_accuracy: 0.8324\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1676 - accuracy: 0.9298 - val_loss: 0.9696 - val_accuracy: 0.8324\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1653 - accuracy: 0.9312 - val_loss: 0.9226 - val_accuracy: 0.8380\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1625 - accuracy: 0.9270 - val_loss: 1.0058 - val_accuracy: 0.8324\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9298 - val_loss: 0.9399 - val_accuracy: 0.8324\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.9326 - val_loss: 0.9405 - val_accuracy: 0.8324\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1643 - accuracy: 0.9298 - val_loss: 0.9962 - val_accuracy: 0.8268\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1634 - accuracy: 0.9368 - val_loss: 1.0094 - val_accuracy: 0.8268\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9284 - val_loss: 0.9618 - val_accuracy: 0.8324\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1680 - accuracy: 0.9298 - val_loss: 0.9800 - val_accuracy: 0.8156\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1699 - accuracy: 0.9326 - val_loss: 0.9801 - val_accuracy: 0.8324\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9284 - val_loss: 1.0061 - val_accuracy: 0.8324\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1717 - accuracy: 0.9326 - val_loss: 1.0455 - val_accuracy: 0.8212\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9326 - val_loss: 1.0659 - val_accuracy: 0.8324\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1602 - accuracy: 0.9382 - val_loss: 1.0342 - val_accuracy: 0.8156\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9340 - val_loss: 0.9512 - val_accuracy: 0.8492\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1714 - accuracy: 0.9284 - val_loss: 0.9537 - val_accuracy: 0.8324\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9312 - val_loss: 0.9475 - val_accuracy: 0.8212\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.9298 - val_loss: 1.0177 - val_accuracy: 0.8045\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1652 - accuracy: 0.9340 - val_loss: 1.0404 - val_accuracy: 0.8101\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9340 - val_loss: 1.0271 - val_accuracy: 0.8268\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1546 - accuracy: 0.9340 - val_loss: 1.0507 - val_accuracy: 0.8324\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9284 - val_loss: 1.0424 - val_accuracy: 0.8268\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.9326 - val_loss: 1.0345 - val_accuracy: 0.8212\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9284 - val_loss: 0.9432 - val_accuracy: 0.8380\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9354 - val_loss: 1.0407 - val_accuracy: 0.8268\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1627 - accuracy: 0.9312 - val_loss: 1.0323 - val_accuracy: 0.8436\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1667 - accuracy: 0.9284 - val_loss: 1.0186 - val_accuracy: 0.8324\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9312 - val_loss: 1.0101 - val_accuracy: 0.8324\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1654 - accuracy: 0.9270 - val_loss: 0.9818 - val_accuracy: 0.8268\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9340 - val_loss: 0.9698 - val_accuracy: 0.8268\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.9270 - val_loss: 1.0152 - val_accuracy: 0.8324\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1592 - accuracy: 0.9340 - val_loss: 0.9970 - val_accuracy: 0.8324\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1614 - accuracy: 0.9312 - val_loss: 1.0193 - val_accuracy: 0.8156\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1526 - accuracy: 0.9326 - val_loss: 1.0915 - val_accuracy: 0.8324\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1538 - accuracy: 0.9340 - val_loss: 1.0808 - val_accuracy: 0.8268\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9340 - val_loss: 1.0884 - val_accuracy: 0.8268\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9368 - val_loss: 1.1188 - val_accuracy: 0.8101\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9396 - val_loss: 1.1395 - val_accuracy: 0.8268\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.9298 - val_loss: 1.1020 - val_accuracy: 0.8268\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1686 - accuracy: 0.9326 - val_loss: 1.0805 - val_accuracy: 0.8436\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1578 - accuracy: 0.9326 - val_loss: 1.0345 - val_accuracy: 0.8212\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1799 - accuracy: 0.9326 - val_loss: 1.0699 - val_accuracy: 0.8324\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.9326 - val_loss: 1.0010 - val_accuracy: 0.8380\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1636 - accuracy: 0.9270 - val_loss: 1.0233 - val_accuracy: 0.8380\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9312 - val_loss: 1.0293 - val_accuracy: 0.8380\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1569 - accuracy: 0.9368 - val_loss: 1.0805 - val_accuracy: 0.8268\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9326 - val_loss: 1.1044 - val_accuracy: 0.8436\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1512 - accuracy: 0.9326 - val_loss: 1.0907 - val_accuracy: 0.8380\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9326 - val_loss: 1.1130 - val_accuracy: 0.8380\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1573 - accuracy: 0.9298 - val_loss: 1.0856 - val_accuracy: 0.8380\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9368 - val_loss: 1.1083 - val_accuracy: 0.8380\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.9298 - val_loss: 1.0942 - val_accuracy: 0.8212\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9340 - val_loss: 1.0856 - val_accuracy: 0.8380\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1447 - accuracy: 0.9410 - val_loss: 1.1523 - val_accuracy: 0.8212\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1501 - accuracy: 0.9424 - val_loss: 1.1854 - val_accuracy: 0.8156\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1480 - accuracy: 0.9354 - val_loss: 1.1331 - val_accuracy: 0.8156\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1509 - accuracy: 0.9354 - val_loss: 1.1560 - val_accuracy: 0.8380\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.9368 - val_loss: 1.1550 - val_accuracy: 0.8324\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1619 - accuracy: 0.9270 - val_loss: 1.1172 - val_accuracy: 0.8380\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1551 - accuracy: 0.9326 - val_loss: 1.1452 - val_accuracy: 0.8436\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.9326 - val_loss: 1.1376 - val_accuracy: 0.8156\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1502 - accuracy: 0.9410 - val_loss: 1.1811 - val_accuracy: 0.8268\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1500 - accuracy: 0.9298 - val_loss: 1.2031 - val_accuracy: 0.8380\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9340 - val_loss: 1.1909 - val_accuracy: 0.8436\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1465 - accuracy: 0.9326 - val_loss: 1.1763 - val_accuracy: 0.8380\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1530 - accuracy: 0.9340 - val_loss: 1.2073 - val_accuracy: 0.8324\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2158 - accuracy: 0.9199 - val_loss: 1.0527 - val_accuracy: 0.8324\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.9129 - val_loss: 0.8345 - val_accuracy: 0.8324\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9326 - val_loss: 0.9526 - val_accuracy: 0.8324\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1519 - accuracy: 0.9354 - val_loss: 0.9699 - val_accuracy: 0.8380\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1531 - accuracy: 0.9312 - val_loss: 1.0288 - val_accuracy: 0.8268\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1494 - accuracy: 0.9326 - val_loss: 0.9877 - val_accuracy: 0.8380\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1495 - accuracy: 0.9312 - val_loss: 1.0481 - val_accuracy: 0.8268\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9354 - val_loss: 1.0622 - val_accuracy: 0.8436\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9326 - val_loss: 1.0441 - val_accuracy: 0.8324\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9312 - val_loss: 1.0492 - val_accuracy: 0.8268\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1510 - accuracy: 0.9326 - val_loss: 1.0721 - val_accuracy: 0.8324\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9340 - val_loss: 1.0602 - val_accuracy: 0.8324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=[titanic_train_prepared.shape[1]]),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(titanic_train_prepared, titanic_labels, epochs=200, batch_size=16, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [],
   "source": [
    "titanic_test_prepared = full_pipeline.transform(test_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[2.63272203e-04],\n       [7.81693980e-02],\n       [5.28526980e-05],\n       [7.22878724e-02],\n       [5.17197810e-02],\n       [1.58897936e-02],\n       [8.89636159e-01],\n       [2.96888652e-06],\n       [9.99987304e-01],\n       [1.31059787e-04],\n       [5.00156097e-02],\n       [8.27250183e-02],\n       [1.00000000e+00],\n       [2.00152940e-07],\n       [1.00000000e+00],\n       [9.98469055e-01],\n       [5.33046958e-04],\n       [2.11237058e-01],\n       [2.27365433e-03],\n       [7.16455588e-06],\n       [9.70086873e-01],\n       [9.93071079e-01],\n       [9.98498559e-01],\n       [9.70983624e-01],\n       [1.00000000e+00],\n       [7.32112038e-10],\n       [1.00000000e+00],\n       [2.00629279e-01],\n       [9.92795050e-01],\n       [1.53304773e-05],\n       [4.88535479e-09],\n       [1.71974771e-05],\n       [7.65161335e-01],\n       [2.08102111e-02],\n       [5.21818222e-03],\n       [1.41992345e-01],\n       [7.67724812e-02],\n       [7.23269908e-03],\n       [2.99965851e-02],\n       [7.16581881e-01],\n       [7.02198744e-09],\n       [9.65426385e-01],\n       [3.42769116e-01],\n       [9.99761403e-01],\n       [1.00000000e+00],\n       [2.75490433e-02],\n       [7.41990423e-03],\n       [6.46390766e-02],\n       [1.00000000e+00],\n       [9.81139136e-04],\n       [3.42429914e-02],\n       [8.23769660e-05],\n       [9.99974012e-01],\n       [1.00000000e+00],\n       [7.87844474e-05],\n       [1.61629363e-10],\n       [5.14974594e-02],\n       [7.05059620e-07],\n       [1.87334052e-04],\n       [1.00000000e+00],\n       [4.84763831e-03],\n       [3.76278520e-01],\n       [8.90373066e-03],\n       [6.91340327e-01],\n       [9.99884784e-01],\n       [7.67766178e-01],\n       [4.99335587e-01],\n       [1.21136568e-02],\n       [9.99994934e-01],\n       [1.00000000e+00],\n       [7.97148824e-01],\n       [5.07347733e-02],\n       [1.95462793e-01],\n       [9.99999762e-01],\n       [1.00000000e+00],\n       [2.33197334e-05],\n       [1.13141023e-01],\n       [1.00000000e+00],\n       [5.21272123e-02],\n       [7.97148824e-01],\n       [1.00000000e+00],\n       [9.99999762e-01],\n       [3.03493619e-01],\n       [5.00156097e-02],\n       [3.76882404e-01],\n       [2.58285090e-05],\n       [8.56962383e-01],\n       [9.35735226e-01],\n       [8.69301021e-01],\n       [1.00000000e+00],\n       [1.51727884e-03],\n       [4.96757515e-02],\n       [1.00000000e+00],\n       [1.13141023e-01],\n       [2.00450867e-02],\n       [2.70985693e-01],\n       [1.00000000e+00],\n       [1.92507580e-01],\n       [9.99077737e-01],\n       [1.94273936e-03],\n       [9.99091744e-01],\n       [9.58827368e-05],\n       [6.46390766e-02],\n       [1.68481216e-01],\n       [9.35373604e-01],\n       [3.64713580e-03],\n       [2.12236820e-03],\n       [6.46390766e-02],\n       [1.17224209e-01],\n       [2.69759651e-02],\n       [6.17421938e-06],\n       [8.69322121e-01],\n       [9.99999940e-01],\n       [4.99404609e-01],\n       [1.00000000e+00],\n       [8.72035045e-04],\n       [1.47776797e-01],\n       [9.99999821e-01],\n       [2.54771709e-01],\n       [9.84902859e-01],\n       [9.99999881e-01],\n       [1.92725405e-04],\n       [9.99998629e-01],\n       [4.82240245e-02],\n       [6.46390766e-02],\n       [2.16188774e-01],\n       [7.44392574e-02],\n       [9.99999523e-01],\n       [8.48929286e-01],\n       [2.04674266e-02],\n       [4.38857218e-03],\n       [2.52624945e-04],\n       [2.64450432e-06],\n       [2.44956953e-03],\n       [4.39203709e-01],\n       [2.42542952e-01],\n       [1.31783634e-01],\n       [6.78641126e-02],\n       [4.72943997e-03],\n       [1.68605210e-20],\n       [2.17899114e-07],\n       [1.00000000e+00],\n       [9.99989867e-01],\n       [7.89703394e-04],\n       [9.94772911e-01],\n       [1.91237837e-06],\n       [2.41600230e-01],\n       [1.49373366e-02],\n       [5.58565199e-01],\n       [1.20607396e-11],\n       [9.99988794e-01],\n       [1.51366636e-01],\n       [2.39397596e-05],\n       [2.09859371e-01],\n       [1.30897062e-08],\n       [2.43162856e-01],\n       [1.00000000e+00],\n       [8.64749193e-01],\n       [9.99997675e-01],\n       [9.99128699e-01],\n       [8.69288981e-01],\n       [1.00000000e+00],\n       [7.14309931e-01],\n       [4.75469455e-02],\n       [4.20774159e-05],\n       [4.22465056e-01],\n       [9.28707778e-01],\n       [5.95388665e-05],\n       [9.64301288e-01],\n       [4.91834059e-03],\n       [4.90485765e-02],\n       [1.36719108e-01],\n       [3.40104941e-03],\n       [1.49716869e-01],\n       [6.11980669e-16],\n       [9.99141753e-01],\n       [9.99995410e-01],\n       [9.71848905e-01],\n       [1.00000000e+00],\n       [9.99953032e-01],\n       [5.21272123e-02],\n       [9.99944031e-01],\n       [1.00000000e+00],\n       [6.46390766e-02],\n       [1.00000000e+00],\n       [2.48458749e-03],\n       [9.99921918e-01],\n       [2.60128570e-03],\n       [4.93267238e-18],\n       [5.43348312e-01],\n       [1.51625881e-02],\n       [5.58648646e-01],\n       [9.99925315e-01],\n       [1.65951438e-04],\n       [1.00000000e+00],\n       [2.32432291e-01],\n       [1.00000000e+00],\n       [9.99898434e-01],\n       [6.79498911e-02],\n       [1.36684917e-03],\n       [9.48265612e-01],\n       [1.00000000e+00],\n       [3.54602293e-04],\n       [1.00000000e+00],\n       [5.91920316e-02],\n       [3.65076074e-03],\n       [7.76905727e-05],\n       [5.30190170e-02],\n       [1.00000000e+00],\n       [2.72201568e-01],\n       [8.50450337e-01],\n       [4.76816259e-02],\n       [3.30348234e-06],\n       [1.00000000e+00],\n       [4.79744311e-04],\n       [1.70103591e-02],\n       [8.69394243e-01],\n       [7.59184093e-09],\n       [1.00000000e+00],\n       [1.13141023e-01],\n       [9.06053424e-01],\n       [3.14446241e-02],\n       [1.00000000e+00],\n       [5.07575721e-02],\n       [8.77957791e-03],\n       [6.19589865e-01],\n       [1.31980583e-01],\n       [8.69301021e-01],\n       [7.88746402e-03],\n       [2.45086215e-02],\n       [1.04850973e-04],\n       [9.99990463e-01],\n       [4.22323287e-01],\n       [6.48083985e-02],\n       [2.22441391e-04],\n       [3.30748931e-02],\n       [9.95883456e-05],\n       [1.88492566e-01],\n       [9.99938369e-01],\n       [1.00000000e+00],\n       [2.07803641e-02],\n       [9.48646724e-01],\n       [2.84307059e-02],\n       [4.99921925e-02],\n       [1.14817594e-04],\n       [1.00000000e+00],\n       [9.99618292e-01],\n       [3.71540736e-08],\n       [9.84902859e-01],\n       [2.77126965e-05],\n       [1.00000000e+00],\n       [1.07545257e-01],\n       [2.99062322e-07],\n       [2.18070336e-02],\n       [3.19343037e-03],\n       [4.90485877e-02],\n       [6.46390766e-02],\n       [1.13141023e-01],\n       [9.99916255e-01],\n       [5.07623181e-02],\n       [3.03873439e-05],\n       [5.07442616e-02],\n       [1.28464028e-01],\n       [1.00000000e+00],\n       [9.96700525e-01],\n       [5.00156097e-02],\n       [1.40476292e-02],\n       [4.90485765e-02],\n       [7.67724812e-02],\n       [2.50201017e-01],\n       [3.74981649e-02],\n       [6.46390766e-02],\n       [9.99999940e-01],\n       [9.99945402e-01],\n       [1.49706513e-01],\n       [9.99946833e-01],\n       [6.95652589e-02],\n       [5.45672083e-06],\n       [2.53236649e-05],\n       [1.33021906e-01],\n       [4.91674524e-03],\n       [1.00000000e+00],\n       [8.69301021e-01],\n       [9.99901891e-01],\n       [9.99998748e-01],\n       [4.78586517e-02],\n       [4.82240245e-02],\n       [5.83287328e-02],\n       [1.27410231e-05],\n       [1.13141023e-01],\n       [2.06903934e-01],\n       [8.88873160e-01],\n       [1.49716869e-01],\n       [9.98174608e-01],\n       [1.07243343e-03],\n       [1.69357941e-01],\n       [1.00000000e+00],\n       [1.53304773e-05],\n       [4.83356304e-02],\n       [3.25078703e-02],\n       [3.51963073e-01],\n       [1.20237969e-01],\n       [1.86446147e-08],\n       [2.11049635e-02],\n       [8.69301021e-01],\n       [1.00000000e+00],\n       [5.12818623e-08],\n       [1.00000000e+00],\n       [3.55742037e-01],\n       [4.57758456e-03],\n       [2.65869439e-01],\n       [2.11943865e-01],\n       [4.91178930e-02],\n       [1.13752452e-04],\n       [1.00000000e+00],\n       [4.97951597e-01],\n       [9.88988817e-01],\n       [2.13248476e-01],\n       [8.99296105e-02],\n       [4.39942232e-05],\n       [1.68481216e-01],\n       [1.08195964e-04],\n       [6.48307381e-03],\n       [9.99991059e-01],\n       [1.00000000e+00],\n       [1.33261770e-01],\n       [1.00000000e+00],\n       [1.91958882e-02],\n       [3.48605754e-05],\n       [2.31544197e-01],\n       [9.70793724e-01],\n       [9.99733746e-01],\n       [1.49706513e-01],\n       [9.59112406e-01],\n       [9.01664719e-02],\n       [5.11979759e-01],\n       [3.67455840e-01],\n       [9.97018690e-10],\n       [2.40867827e-04],\n       [3.04486692e-01],\n       [1.89914927e-01],\n       [3.49432498e-01],\n       [2.49983735e-22],\n       [1.00000000e+00],\n       [3.08615132e-03],\n       [9.99984860e-01],\n       [6.78641126e-02],\n       [3.51047311e-06],\n       [5.46750389e-02],\n       [9.99999404e-01],\n       [1.00000000e+00],\n       [5.91920316e-02],\n       [4.58753630e-06],\n       [6.10396012e-10],\n       [1.00000000e+00],\n       [5.67247629e-01],\n       [1.00000000e+00],\n       [4.99687865e-02],\n       [6.46390766e-02],\n       [9.84804273e-01],\n       [3.89561921e-21],\n       [1.00000000e+00],\n       [1.00000000e+00],\n       [7.22878724e-02],\n       [1.00000000e+00],\n       [4.33474625e-08],\n       [1.64638343e-03],\n       [9.46909785e-01],\n       [2.97650248e-02],\n       [9.97894824e-01],\n       [1.63712740e-01],\n       [1.00000000e+00],\n       [2.60407776e-01],\n       [8.79395604e-01],\n       [1.00000000e+00],\n       [1.00000000e+00],\n       [2.52644932e-05],\n       [2.32337207e-01],\n       [1.09174005e-04],\n       [9.47807077e-09],\n       [6.46390766e-02],\n       [1.34551926e-02],\n       [3.70894015e-01],\n       [6.69382466e-03],\n       [7.06648603e-02],\n       [9.99963939e-01],\n       [2.42704317e-01],\n       [5.03727733e-06],\n       [2.12267623e-03],\n       [1.31401556e-09],\n       [2.39518951e-04],\n       [1.00000000e+00],\n       [9.99760807e-01],\n       [1.76115296e-04],\n       [7.23265281e-10],\n       [9.99995291e-01],\n       [5.38116181e-03],\n       [1.00000000e+00],\n       [7.44497478e-02],\n       [4.73006367e-05],\n       [1.00000000e+00],\n       [1.10246892e-05],\n       [9.99996185e-01],\n       [5.81883013e-01],\n       [9.99997497e-01],\n       [9.99998271e-01],\n       [6.31563365e-02],\n       [5.38449103e-12],\n       [8.69279921e-01],\n       [9.99999762e-01],\n       [8.69301021e-01],\n       [1.00000000e+00],\n       [1.79388180e-01],\n       [1.13141023e-01],\n       [1.00000000e+00],\n       [1.76181093e-01],\n       [1.13141023e-01],\n       [9.99947071e-01]], dtype=float32)"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(titanic_test_prepared)\n",
    "prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [0],\n       [1],\n       [0],\n       [0],\n       [1]])"
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.round().astype('int32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [],
   "source": [
    "neural_submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_data[\"PassengerId\"],\n",
    "    \"Survived\": prediction.round().astype('int32').flatten()\n",
    "})\n",
    "neural_submission.to_csv(\"titanic_submission_clean_neural_more_features.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Look again at the random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=200, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.6s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=20, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=200, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.3s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=220, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.5s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n",
      "[CV] END max_depth=6, max_features=25, n_estimators=250, oob_score=True; total time=   0.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=10, estimator=RandomForestClassifier(random_state=42),\n             param_grid=[{'max_depth': [6], 'max_features': [20, 25],\n                          'n_estimators': [200, 220, 250],\n                          'oob_score': [True]}],\n             verbose=2)"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "num_attribs = [\"Cabin_sum\" ,'Fare']\n",
    "cat_attribs = [\"Sex\", \"Title\", \"Embarked\", 'Fare_Group', \"Age_Group\", \"Family_Type\", \"Pclass\", 'Deck']\n",
    "\n",
    "full_transformer_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs)\n",
    "])\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_adder', FeatureAdder()),\n",
    "    ('feature_selector', FeatureSelector()),\n",
    "    ('transformer', full_transformer_pipeline),\n",
    "])\n",
    "\n",
    "titanic_train_prepared = full_pipeline.fit_transform(titanic_data)\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [200, 220, 250], 'max_features': [20, 25], 'max_depth': [6], 'oob_score':[True]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=10, verbose=2)\n",
    "grid_search.fit(titanic_train_prepared, titanic_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8575031210986266"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(max_depth=6, max_features=25, n_estimators=200,\n                       oob_score=True, random_state=42)"
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(max_depth=6, max_features=25, n_estimators=200,\n                       oob_score=True, random_state=42)"
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest = grid_search.best_estimator_\n",
    "best_forest.fit(titanic_train_prepared, titanic_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1],\n      dtype=int64)"
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test_prepared = full_pipeline.transform(test_data)\n",
    "titanic_predictions_forest = best_forest.predict(titanic_test_prepared)\n",
    "titanic_predictions_forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.2968046008892959, 'Mr'),\n (0.16403392518969107, 'Fare'),\n (0.1046225997143548, 3),\n (0.09730793994210152, 'female'),\n (0.08877689127410013, 'male'),\n (0.025840830095268327, 1),\n (0.02352074304260052, 'Other'),\n (0.021175985867049198, 'Small'),\n (0.020334945820044473, 'Big'),\n (0.016261749085270276, 'Adult'),\n (0.01593554338197907, 'Master'),\n (0.01522658037098654, 'DE'),\n (0.01233917077695509, 'S'),\n (0.011440157622866718, 'Child'),\n (0.011125412690640617, 'Standard'),\n (0.0086432931509413, 2),\n (0.007611517364045636, 'Expensive'),\n (0.007226565559973198, 'C'),\n (0.0071305205298273795, 'Young Adult'),\n (0.0066900287881416715, 'ABC'),\n (0.005960510365325999, 'Old'),\n (0.005066360421312534, 'Very Old'),\n (0.0043256492662989855, 'Luxury'),\n (0.0042538089936097956, 'Alone'),\n (0.004177343774371802, 'Cabin_sum'),\n (0.003131742365449428, 'Q'),\n (0.0030566323845118946, 'Very Big'),\n (0.0026980431697345023, 'Miss'),\n (0.00248205766017053, 'Cheap'),\n (0.0017987010700181397, 'Mrs'),\n (0.0010001493730630896, 'FG')]"
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_categories = list(np.concatenate(full_pipeline['transformer'].named_transformers_[\"cat\"]['category_encoder'].categories_).flat)\n",
    "attributes = num_attribs + one_hot_categories\n",
    "sorted(zip(best_forest.feature_importances_, attributes), reverse=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_data[\"PassengerId\"],\n",
    "    \"Survived\": titanic_predictions_forest\n",
    "})\n",
    "submission.to_csv(\"titanic_submission_forest_with_deck.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
